{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e6024",
   "metadata": {},
   "source": [
    "# About this example\n",
    "\n",
    "This example shows how you can run a Facenet model to perform Facial Recognition with confidentiality guarantees. \n",
    "\n",
    "By using BlindAI, people can send data for the AI to analyze their biometric data without having to fear privacy leaks.\n",
    "\n",
    "Facenet is a state-of-the art ResNet model for Facial Recogntion. You can learn more about it on [Facenet repository](https://github.com/timesler/facenet-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962be97",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda973b",
   "metadata": {},
   "source": [
    "Install the dependencies this example needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78255e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[onnx] torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889651b",
   "metadata": {},
   "source": [
    "Install the Facenet-pytorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43829608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: facenet-pytorch in /home/dhuynh/.local/lib/python3.8/site-packages (2.5.2)\n",
      "Requirement already satisfied: torchvision in /home/dhuynh/.local/lib/python3.8/site-packages (from facenet-pytorch) (0.11.3+cpu)\n",
      "Requirement already satisfied: pillow in /home/dhuynh/.local/lib/python3.8/site-packages (from facenet-pytorch) (9.0.1)\n",
      "Requirement already satisfied: numpy in /home/dhuynh/.local/lib/python3.8/site-packages (from facenet-pytorch) (1.21.0)\n",
      "Requirement already satisfied: requests in /home/dhuynh/.local/lib/python3.8/site-packages (from facenet-pytorch) (2.27.1)\n",
      "Requirement already satisfied: torch==1.10.2 in /home/dhuynh/.local/lib/python3.8/site-packages (from torchvision->facenet-pytorch) (1.10.2+cpu)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->facenet-pytorch) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests->facenet-pytorch) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/dhuynh/.local/lib/python3.8/site-packages (from requests->facenet-pytorch) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->facenet-pytorch) (1.25.8)\n",
      "Requirement already satisfied: typing-extensions in /home/dhuynh/.local/lib/python3.8/site-packages (from torch==1.10.2->torchvision->facenet-pytorch) (4.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd99a83",
   "metadata": {},
   "source": [
    "Install the latest version of BlindAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02339f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blindai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa298c",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1bc79",
   "metadata": {},
   "source": [
    "The first step here is to prepare the model to perform facial recognition. \n",
    "\n",
    "To make it simpler, we will do an example where we will hardcode the database of biometric templates in the neural network itself. This works if the database of people to identify is fixed. For more dynamic workload, BlindAI can be adapted to suit this use case but we will not cover it here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd5b71",
   "metadata": {},
   "source": [
    "First we load the pretrained Facenet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fdf9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48299a01",
   "metadata": {},
   "source": [
    "We then download the people that will serve as our biometric database. The goal here is to use a neural network to see if a new person to be identified belongs to one of the three people registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3494e052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-06 16:24:34--  https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_0.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4853 (4.7K) [image/jpeg]\n",
      "Saving to: ‘woman_0.jpg’\n",
      "\n",
      "woman_0.jpg         100%[===================>]   4.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-06-06 16:24:34 (32.3 MB/s) - ‘woman_0.jpg’ saved [4853/4853]\n",
      "\n",
      "--2022-06-06 16:24:34--  https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_1.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4019 (3.9K) [image/jpeg]\n",
      "Saving to: ‘woman_1.jpg’\n",
      "\n",
      "woman_1.jpg         100%[===================>]   3.92K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-06-06 16:24:34 (55.7 MB/s) - ‘woman_1.jpg’ saved [4019/4019]\n",
      "\n",
      "--2022-06-06 16:24:34--  https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_2.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3872 (3.8K) [image/jpeg]\n",
      "Saving to: ‘woman_2.jpg’\n",
      "\n",
      "woman_2.jpg         100%[===================>]   3.78K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-06-06 16:24:34 (34.7 MB/s) - ‘woman_2.jpg’ saved [3872/3872]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_0.jpg\n",
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_1.jpg\n",
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0345f0",
   "metadata": {},
   "source": [
    "We can have a look at our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cc2fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "files = [f\"woman_{i}.jpg\" for i in range(3)]\n",
    "Image.open(files[0]).show(), Image.open(files[1]).show(), Image.open(files[2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7a98b",
   "metadata": {},
   "source": [
    "Here we will do the enrollment phase, i.e. extract a template from each person, and store it. Those templates will be used as references to compute a similarity score when someone new comes in to be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2fd2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for file in files:\n",
    "    # We open each file and preprocess it\n",
    "    im = Image.open(file)\n",
    "    im = torch.tensor(np.asarray(im)).permute(2,0,1).unsqueeze(0) / 128.0 - 1\n",
    "    \n",
    "    # We make the tensor go through the ResNet to extract a template\n",
    "    embedding = resnet(im)\n",
    "    embeddings.append(embedding.squeeze(0))\n",
    "    \n",
    "# We stack everything in a matrix\n",
    "embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e6466",
   "metadata": {},
   "source": [
    "Because the scoring will be done through a dot product of a new candidate template with the registered templates, we can implement this scoring as a matrix multiplication between the registered tempalte and the new template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ef9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the scoring layer with a matrix multiplication\n",
    "scoring_layer = nn.Linear(512, 3, bias=False)\n",
    "\n",
    "# Store the computed embeddings inside\n",
    "scoring_layer.weight.data = embeddings\n",
    "\n",
    "full_network = nn.Sequential(\n",
    "    resnet,\n",
    "    scoring_layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad40ef9",
   "metadata": {},
   "source": [
    "Before sending our model to BlindAI, we will how it performs in practice.\n",
    "\n",
    "Let's download a test set, containing a different picture of the second woman we registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a480e5a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-06 16:43:53--  https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_test.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3856 (3.8K) [image/jpeg]\n",
      "Saving to: ‘woman_test.jpg.1’\n",
      "\n",
      "woman_test.jpg.1    100%[===================>]   3.77K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-06-06 16:43:54 (46.9 MB/s) - ‘woman_test.jpg.1’ saved [3856/3856]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mithril-security/blindai/master/examples/facenet/woman_test.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24bbc5",
   "metadata": {},
   "source": [
    "We can see below that the two pictures are indeed from the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031831fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_im = Image.open(\"woman_test.jpg\")\n",
    "test_im.show(), Image.open(\"woman_1.jpg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f42b5",
   "metadata": {},
   "source": [
    "We can now apply our full network, which will extract a template from the test image, and compute a dot product between the new templates and the registered templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f55849",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = torch.tensor(np.asarray(test_im)).permute(2,0,1).unsqueeze(0) / 128.0 - 1\n",
    "\n",
    "scores = full_network(test_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea6b8a",
   "metadata": {},
   "source": [
    "We can see that the scores reflect the truth: the dot product of the embeddings of the test image with the first and third women are low, while the score is high with the second woman. This makes sense, as the neural network was trained to provide a high score for pictures of the same person, and make the score low for different people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3833d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0418,  0.7356,  0.0715]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012be8b",
   "metadata": {},
   "source": [
    "Now we can export the model to be fed to BlindAI to deploy it with privacy guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988ecc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(full_network,               # model being run\n",
    "                  test_im,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"facenet.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ccb5",
   "metadata": {},
   "source": [
    "# Deployment on BlindAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2366fb",
   "metadata": {},
   "source": [
    "Please make sure the **server is running**. To launch the server, refer to the [Launching the server](https://docs.mithrilsecurity.io/getting-started/quick-start/run-the-blindai-server) documentation page. \n",
    "\n",
    "If you have followed the steps and have the Docker image ready, this mean you simply have to run `docker run -it -p 50051:50051 -p 50052:50052 mithrilsecuritysas/blindai-server-sim:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc49a9",
   "metadata": {},
   "source": [
    "So the first thing we need to do is to connect securely to the BlindAI server instance. Here we will use simulation mode for ease of use. This means that we do not leverage the hardware security propertiers of secure enclaves, but we do not need to run the Docker image with a specific hardware.\n",
    "\n",
    "If you wish to run this example in hardware mode, you need to prepare the `host_server.pem` and `policy.toml` files. Learn more on the [Deploy on Hardware](https://docs.mithrilsecurity.io/getting-started/deploy-on-hardware) documentation page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155ddc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Untrusted server certificate check bypassed\n",
      "WARNING:root:Attestation process is bypassed: running without requesting and checking attestation\n"
     ]
    }
   ],
   "source": [
    "from blindai.client import BlindAiClient, ModelDatumType\n",
    "\n",
    "# Launch client\n",
    "client = BlindAiClient()\n",
    "\n",
    "# Simulation mode\n",
    "client.connect_server(addr=\"localhost\", simulation=True)\n",
    "\n",
    "# Hardware mode\n",
    "# client.connect_server(addr=\"localhost\", policy=\"./policy.toml\", certificate=\"./host_server.pem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d5725",
   "metadata": {},
   "source": [
    "Then, upload the model inside the BlindAI server. This simply means uploading the ONNX file created before.\n",
    "\n",
    "When uploading the model, we have to precise the shape of the input and the data type. \n",
    "\n",
    "In this case, because we use a ResNet model, we will need to send floats for the facial data. As the outputs are scores, we will accept floats as well for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f605f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<blindai.client.UploadModelResponse at 0x7fe5fc1ca460>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upload_model(model=\"./facenet.onnx\", shape=test_im.shape, \n",
    "                    dtype=ModelDatumType.F32, dtype_out=ModelDatumType.F32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501791e6",
   "metadata": {},
   "source": [
    "# Sending data for confidential prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3896eb0",
   "metadata": {},
   "source": [
    "Now it's time to check it's working live!\n",
    "\n",
    "We will just prepare some input for the model inside the secure enclave of BlindAI to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56eb415",
   "metadata": {},
   "source": [
    "First we prepare our input data, the test image we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15799bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "test_im = Image.open(\"woman_test.jpg\")\n",
    "test_im = torch.tensor(np.asarray(test_im)).permute(2,0,1).unsqueeze(0) / 128.0 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73769b",
   "metadata": {},
   "source": [
    "Now we can send the audio data to be processed confidentially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b48baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.run_model(test_im.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea276d7",
   "metadata": {},
   "source": [
    "As we can see below, the results are quite similar from the regular inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f43c74dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04181796684861183, 0.7355775237083435, 0.0715053379535675]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee978593",
   "metadata": {},
   "source": [
    "Et voila! We have been able to apply a start of the art model for facial recognition, without ever having to show the data in clear to the people operating the service!\n",
    "\n",
    "If you have liked this example, do not hesitate to drop a star on our [GitHub](https://github.com/mithril-security/blindai) and chat with us on our [Discord](https://discord.gg/TxEHagpWd4)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
