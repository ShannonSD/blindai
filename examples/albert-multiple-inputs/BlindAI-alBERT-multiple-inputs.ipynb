{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9f250e",
   "metadata": {},
   "source": [
    "# About this example\n",
    "\n",
    "This example shows how you can deploy ALBERT model to analyze confidential text for sentiment analysis. The model will be left untrained for demonstration purposes. We could finetune it on positive/negative samples before deploying it.\n",
    "\n",
    "By using BlindAI, people can send data for the AI to analyze sensitive text without having to fear privacy leaks.\n",
    "\n",
    "ALBERT is a state of the art Transformers model for NLP. You can learn more about it on this [Hugging Face page](https://huggingface.co/docs/transformers/model_doc/albert).\n",
    "\n",
    "More information on this use case can be found on our blog post [Deploy Transformers models with confidentiality](https://blog.mithrilsecurity.io/transformers-with-confidentiality/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334419ad",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9cfaa",
   "metadata": {},
   "source": [
    "Install the dependencies this example needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6625af10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: transformers[onnx] in /usr/local/lib/python3.6/dist-packages (4.18.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.10.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (0.4.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (0.0.53)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (4.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (3.4.1)\n",
      "Requirement already satisfied: onnxconverter-common in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (1.9.0)\n",
      "Requirement already satisfied: onnxruntime-tools>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (1.7.0)\n",
      "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (1.10.1)\n",
      "Requirement already satisfied: onnxruntime>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from transformers[onnx]) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from onnxruntime>=1.4.0->transformers[onnx]) (1.12)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime>=1.4.0->transformers[onnx]) (3.19.4)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]) (1.11.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]) (5.9.1)\n",
      "Requirement already satisfied: py3nvml in /usr/local/lib/python3.6/dist-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]) (0.2.7)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.6/dist-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]) (8.0.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.6/dist-packages (from onnxruntime-tools>=1.4.2->transformers[onnx]) (15.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->transformers[onnx]) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm>=4.27->transformers[onnx]) (5.2.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers[onnx]) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[onnx]) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[onnx]) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers[onnx]) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[onnx]) (1.26.9)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[onnx]) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers[onnx]) (1.11.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[onnx]) (8.0.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.6/dist-packages (from coloredlogs->onnxruntime-tools>=1.4.2->transformers[onnx]) (10.0)\n",
      "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from py3nvml->onnxruntime-tools>=1.4.2->transformers[onnx]) (0.13.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[onnx] torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008593de",
   "metadata": {},
   "source": [
    "Install the latest version of BlindAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc9033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install blindai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553770d3",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8e595",
   "metadata": {},
   "source": [
    "In this first step we will export a standard Hugging Face ALBERT model to an ONNX file, as BlindAI accepts only ONNX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf6fcc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./albert/tokenizer_config.json',\n",
       " './albert/special_tokens_map.json',\n",
       " './albert/tokenizer.json')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_name = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "text = \"Paris is the [MASK] of France.\"\n",
    "tokenizer_output = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokenizer_output[\"input_ids\"]\n",
    "attention_mask = tokenizer_output[\"attention_mask\"]\n",
    "token_type_ids = tokenizer_output[\"token_type_ids\"]\n",
    "\n",
    "dynamic_axes = {\n",
    "    0: \"batch\",\n",
    "    1: \"seq\",\n",
    "}\n",
    "\n",
    "output_dir = \"./albert\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (input_ids, attention_mask, token_type_ids),\n",
    "    os.path.join(output_dir, \"model.onnx\"),\n",
    "    input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": dynamic_axes,\n",
    "        \"attention_mask\": dynamic_axes,\n",
    "        \"token_type_ids\": dynamic_axes,\n",
    "        \"logits\": dynamic_axes,\n",
    "    },\n",
    "    opset_version=13,\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8d46a",
   "metadata": {},
   "source": [
    "# Deployment on BlindAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b5ad38",
   "metadata": {},
   "source": [
    "Please make sure the **server is running**. To launch the server, refer to the [Launching the server](https://docs.mithrilsecurity.io/getting-started/quick-start/run-the-blindai-server) documentation page. \n",
    "\n",
    "If you have followed the steps and have the Docker image ready, this mean you simply have to run `docker run -it -p 50051:50051 -p 50052:50052 mithrilsecuritysas/blindai-server-sim:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68573c53",
   "metadata": {},
   "source": [
    "So the first thing we need to do is to connect securely to the BlindAI server instance. Here we will use simulation mode for ease of use. This means that we do not leverage the hardware security propertiers of secure enclaves, but we do not need to run the Docker image with a specific hardware.\n",
    "\n",
    "If you wish to run this example in hardware mode, you need to prepare the `host_server.pem` and `policy.toml` files. Learn more on the [Deploy on Hardware](https://docs.mithrilsecurity.io/getting-started/deploy-on-hardware) documentation page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "155ddc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Untrusted server certificate check bypassed\n",
      "WARNING:root:Attestation process is bypassed: running without requesting and checking attestation\n"
     ]
    }
   ],
   "source": [
    "from blindai.client import BlindAiClient, ModelDatumType\n",
    "\n",
    "# Launch client\n",
    "client = BlindAiClient()\n",
    "\n",
    "# Simulation mode\n",
    "client.connect_server(addr=\"localhost\", simulation=True)\n",
    "\n",
    "# Hardware mode\n",
    "# client.connect_server(addr=\"localhost\", policy=\"./policy.toml\", certificate=\"./host_server.pem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d5725",
   "metadata": {},
   "source": [
    "Then, upload the model inside the BlindAI server. This simply means uploading the ONNX file created before.\n",
    "\n",
    "When uploading the model, we have to precise the shape of the input and the data type. In this case, because we use Transformers model with tokens, we simply need to send the indices of the tokens, i.e. integers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e835b",
   "metadata": {},
   "source": [
    "And in the case of a multiple input model like ALBERT, we have to provide each input and its datum type in a list just as provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6bdb82e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<blindai.client.UploadModelResponse at 0x7f1d45ac6b00>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_inputs = [\n",
    "    [input_ids.shape, ModelDatumType.I64],\n",
    "    [token_type_ids.shape, ModelDatumType.I64],\n",
    "    [attention_mask.shape, ModelDatumType.I64]\n",
    "]\n",
    "\n",
    "tensor_outputs = ModelDatumType.F32\n",
    "client.upload_model(model=\"./albert/model.onnx\", tensor_inputs=tensor_inputs, tensor_outputs=tensor_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5c14e",
   "metadata": {},
   "source": [
    "# Sending data for confidential prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f17a9f",
   "metadata": {},
   "source": [
    "Now it's time to check it's working live!\n",
    "\n",
    "We will just prepare the inputs for the model inside the secure enclave of BlindAI to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "37386717",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text)[\"input_ids\"]\n",
    "token_type = tokenizer(text)[\"token_type_ids\"]\n",
    "attention = tokenizer(text)[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c49e3e",
   "metadata": {},
   "source": [
    "Now we can get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ac8b4612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.296055793762207, 1.568918228149414, 2.0557966232299805, 4.909221172332764, 7.076786518096924, -1.025282382965088, 4.391934394836426, -2.521080255508423, 0.37863948941230774, 3.25677227973938, 6.41582727432251, 2.833071231842041, -7.473093509674072, -2.3739407062530518, 8.971452713012695, 3.583134174346924, 7.851095676422119, 4.262254238128662, 2.3475804328918457, 9.010588645935059]\n"
     ]
    }
   ],
   "source": [
    "response = client.run_model([inputs, token_type, attention])\n",
    "print(response.output[:20]) # Reducing the output if it's very long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e609cfb",
   "metadata": {},
   "source": [
    "Here we can compare the results against the original prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45c535d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.294389724731445, 1.5712192058563232, 2.0538954734802246, 4.907917499542236, 7.075641632080078, -1.021797776222229, 4.391971588134766, -2.5168683528900146, 0.3809121549129486, 3.2570388317108154, 6.416499137878418, 2.8358120918273926, -7.472112655639648, -2.371373414993286, 8.972518920898438, 3.582233428955078, 7.852456092834473, 4.262726783752441, 2.347175359725952, 9.01127815246582]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model(torch.tensor(inputs).unsqueeze(0), torch.tensor(\n",
    "    token_type).unsqueeze(0), torch.tensor(attention).unsqueeze(0)).logits.detach()\n",
    "\n",
    "print(outputs[0][0].tolist()[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1d65f",
   "metadata": {},
   "source": [
    "Et voila! We have been able to apply a start of the art model of NLP, without ever having to show the data in clear to the people operating the service!\n",
    "\n",
    "If you have liked this example, do not hesitate to drop a star on our [GitHub](https://github.com/mithril-security/blindai) and chat with us on our [Discord](https://discord.gg/TxEHagpWd4)!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
